{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional AI Sequence Embedding Recommendation Model (Caser)\n",
    "\n",
    "#### Acknowledgement\n",
    "\n",
    "Tang, J. and Wang, K. (2018). Personalized top-n sequential recommendation via convolutional sequence embedding. In ACM International Conference on Web Search and Data Mining.\n",
    "\n",
    "This notebook is based on the content of the GitHub repository (https://github.com/graytowne/caser_pytorch). Portions of the code presented here are adapted or directly borrowed from the repository, with modifications made for specific purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import later used packages\n",
    "import pandas as pd\n",
    "\n",
    "# custom functions see .py files\n",
    "# by Caser Paper author\n",
    "from train_caser import Recommender\n",
    "from interactions import Interactions\n",
    "from utils import *\n",
    "\n",
    "# selfmade\n",
    "from Rec_split import rec_split\n",
    "from Kendall_distance import kendall_distance_with_penalty\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loding and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "df = pd.read_csv('data/ml_1M_full.csv')\n",
    "df.drop(columns=['Gender', 'Age', 'Occupation', 'Genre'], inplace=True)\n",
    "\n",
    "#split data\n",
    "train_df, val, test = rec_split(df, 'User', 'Timestamp', train_share=0.7, val_share=0.15)\n",
    "\n",
    "#transform rating to binary\n",
    "train_df['Rating'] = train_df['Rating'].apply(lambda x: 1 if x in [4, 5] else 0)\n",
    "val['Rating'] = val['Rating'].apply(lambda x: 1 if x in [4, 5] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as txt \n",
    "train_df.to_csv('data/train.txt', sep='\\t', header=False, index=False)\n",
    "val.to_csv('data/validation.txt', sep='\\t', header=False, index=False)\n",
    "test.to_csv('data/test.txt', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data arguments\n",
    "train_root = 'data/train.txt'\n",
    "val_root = 'data/validation.txt'\n",
    "L = 5\n",
    "T =3\n",
    "\n",
    "# load dataset\n",
    "train = Interactions(train_root)\n",
    "# transform triplets to sequence representation\n",
    "train.to_sequence(L, T)\n",
    "\n",
    "val = Interactions(val_root,\n",
    "                    user_map=train.user_map,\n",
    "                    item_map=train.item_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "# train arguments\n",
    "epochs = [30, 50, 70] #default 50\n",
    "seed = 123\n",
    "batch_size = 512\n",
    "learning_rate = [1e-3, 1e-4]\n",
    "l2 = 1e-6\n",
    "neg_samples = 3\n",
    "use_cuda = False\n",
    "\n",
    "# model dependent arguments\n",
    "d = 50\n",
    "nhs = [8, 16, 32]\n",
    "nvs = [2, 4, 8]\n",
    "drop = 0.5\n",
    "ac_conv ='relu'\n",
    "ac_fc = 'relu'\n",
    "\n",
    "# set seed\n",
    "set_seed(seed, cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "# generate empty dataframe to save results\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for nh in nhs:\n",
    "        for nv in nvs:\n",
    "            for lr in learning_rate:\n",
    "                # train the model for each hyperparameter combination\n",
    "                model = Recommender(n_iter=epoch,\n",
    "                                    batch_size=batch_size,\n",
    "                                    learning_rate=lr,\n",
    "                                    l2=l2,\n",
    "                                    neg_samples=neg_samples,\n",
    "                                    L = L,\n",
    "                                    dims = d,\n",
    "                                    nh = nh,\n",
    "                                    nv = nv,\n",
    "                                    drop = drop,\n",
    "                                    ac_conv = ac_conv,\n",
    "                                    ac_fc = ac_fc,\n",
    "                                    use_cuda=use_cuda)\n",
    "                # save results of a specific hyperparameter combination\n",
    "                result = model.fit(train, val, verbose=False)\n",
    "\n",
    "                # add result to dataframe with all results\n",
    "                results = pd.concat([results, pd.DataFrame(result, index=[0])], ignore_index=True)\n",
    "                results.to_csv('results/Caser_hyperparameter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>MAP</th>\n",
       "      <th>precision@1</th>\n",
       "      <th>precision@5</th>\n",
       "      <th>precision@10</th>\n",
       "      <th>recall@1</th>\n",
       "      <th>recall@5</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>learnig rate</th>\n",
       "      <th>Number of Epochs</th>\n",
       "      <th>nh</th>\n",
       "      <th>nv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.219850</td>\n",
       "      <td>0.174431</td>\n",
       "      <td>0.266225</td>\n",
       "      <td>0.221589</td>\n",
       "      <td>0.199934</td>\n",
       "      <td>0.022577</td>\n",
       "      <td>0.090150</td>\n",
       "      <td>0.158683</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.199758</td>\n",
       "      <td>0.173346</td>\n",
       "      <td>0.252483</td>\n",
       "      <td>0.222781</td>\n",
       "      <td>0.200132</td>\n",
       "      <td>0.021194</td>\n",
       "      <td>0.091621</td>\n",
       "      <td>0.157798</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>70</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.205151</td>\n",
       "      <td>0.172895</td>\n",
       "      <td>0.251656</td>\n",
       "      <td>0.219470</td>\n",
       "      <td>0.197450</td>\n",
       "      <td>0.022133</td>\n",
       "      <td>0.090582</td>\n",
       "      <td>0.156288</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.204445</td>\n",
       "      <td>0.172397</td>\n",
       "      <td>0.253642</td>\n",
       "      <td>0.220331</td>\n",
       "      <td>0.197583</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>0.090299</td>\n",
       "      <td>0.155358</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.222707</td>\n",
       "      <td>0.172349</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.221623</td>\n",
       "      <td>0.197947</td>\n",
       "      <td>0.021052</td>\n",
       "      <td>0.089637</td>\n",
       "      <td>0.156278</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.202228</td>\n",
       "      <td>0.172163</td>\n",
       "      <td>0.240066</td>\n",
       "      <td>0.219768</td>\n",
       "      <td>0.197781</td>\n",
       "      <td>0.020931</td>\n",
       "      <td>0.091147</td>\n",
       "      <td>0.156960</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.326347</td>\n",
       "      <td>0.172091</td>\n",
       "      <td>0.267881</td>\n",
       "      <td>0.226126</td>\n",
       "      <td>0.204818</td>\n",
       "      <td>0.021831</td>\n",
       "      <td>0.087007</td>\n",
       "      <td>0.153871</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.198875</td>\n",
       "      <td>0.172074</td>\n",
       "      <td>0.258775</td>\n",
       "      <td>0.219139</td>\n",
       "      <td>0.198046</td>\n",
       "      <td>0.022322</td>\n",
       "      <td>0.090946</td>\n",
       "      <td>0.156685</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.214238</td>\n",
       "      <td>0.172060</td>\n",
       "      <td>0.248510</td>\n",
       "      <td>0.220497</td>\n",
       "      <td>0.198692</td>\n",
       "      <td>0.020567</td>\n",
       "      <td>0.090722</td>\n",
       "      <td>0.157060</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.217256</td>\n",
       "      <td>0.172015</td>\n",
       "      <td>0.251490</td>\n",
       "      <td>0.219172</td>\n",
       "      <td>0.197152</td>\n",
       "      <td>0.021837</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>0.155269</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.215491</td>\n",
       "      <td>0.171827</td>\n",
       "      <td>0.259437</td>\n",
       "      <td>0.219702</td>\n",
       "      <td>0.197086</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.089523</td>\n",
       "      <td>0.155440</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.193528</td>\n",
       "      <td>0.171692</td>\n",
       "      <td>0.245033</td>\n",
       "      <td>0.220265</td>\n",
       "      <td>0.198013</td>\n",
       "      <td>0.021630</td>\n",
       "      <td>0.091602</td>\n",
       "      <td>0.158498</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.208136</td>\n",
       "      <td>0.171597</td>\n",
       "      <td>0.244536</td>\n",
       "      <td>0.219404</td>\n",
       "      <td>0.195977</td>\n",
       "      <td>0.021041</td>\n",
       "      <td>0.090257</td>\n",
       "      <td>0.157258</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.195588</td>\n",
       "      <td>0.171577</td>\n",
       "      <td>0.251656</td>\n",
       "      <td>0.220066</td>\n",
       "      <td>0.196623</td>\n",
       "      <td>0.021476</td>\n",
       "      <td>0.090353</td>\n",
       "      <td>0.156633</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.199088</td>\n",
       "      <td>0.171433</td>\n",
       "      <td>0.239735</td>\n",
       "      <td>0.221755</td>\n",
       "      <td>0.198758</td>\n",
       "      <td>0.020525</td>\n",
       "      <td>0.090818</td>\n",
       "      <td>0.156942</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.211691</td>\n",
       "      <td>0.171403</td>\n",
       "      <td>0.243543</td>\n",
       "      <td>0.219470</td>\n",
       "      <td>0.197848</td>\n",
       "      <td>0.020612</td>\n",
       "      <td>0.090948</td>\n",
       "      <td>0.156670</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.200608</td>\n",
       "      <td>0.171349</td>\n",
       "      <td>0.246026</td>\n",
       "      <td>0.219636</td>\n",
       "      <td>0.195662</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.090865</td>\n",
       "      <td>0.157418</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222950</td>\n",
       "      <td>0.170794</td>\n",
       "      <td>0.243709</td>\n",
       "      <td>0.219040</td>\n",
       "      <td>0.198725</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>0.089695</td>\n",
       "      <td>0.156081</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.213339</td>\n",
       "      <td>0.170418</td>\n",
       "      <td>0.242384</td>\n",
       "      <td>0.216556</td>\n",
       "      <td>0.195977</td>\n",
       "      <td>0.021081</td>\n",
       "      <td>0.090743</td>\n",
       "      <td>0.157533</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.216224</td>\n",
       "      <td>0.170389</td>\n",
       "      <td>0.248179</td>\n",
       "      <td>0.217119</td>\n",
       "      <td>0.195017</td>\n",
       "      <td>0.021464</td>\n",
       "      <td>0.089973</td>\n",
       "      <td>0.154856</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.229905</td>\n",
       "      <td>0.170082</td>\n",
       "      <td>0.251325</td>\n",
       "      <td>0.216722</td>\n",
       "      <td>0.194983</td>\n",
       "      <td>0.020963</td>\n",
       "      <td>0.088324</td>\n",
       "      <td>0.154983</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.234054</td>\n",
       "      <td>0.169893</td>\n",
       "      <td>0.254470</td>\n",
       "      <td>0.219404</td>\n",
       "      <td>0.194702</td>\n",
       "      <td>0.021871</td>\n",
       "      <td>0.088543</td>\n",
       "      <td>0.153028</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.193464</td>\n",
       "      <td>0.169781</td>\n",
       "      <td>0.245695</td>\n",
       "      <td>0.220298</td>\n",
       "      <td>0.197318</td>\n",
       "      <td>0.020465</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>0.155312</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.337584</td>\n",
       "      <td>0.169671</td>\n",
       "      <td>0.258113</td>\n",
       "      <td>0.225298</td>\n",
       "      <td>0.201424</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.087813</td>\n",
       "      <td>0.151361</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>70</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.327295</td>\n",
       "      <td>0.169015</td>\n",
       "      <td>0.256954</td>\n",
       "      <td>0.221722</td>\n",
       "      <td>0.201424</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>0.086633</td>\n",
       "      <td>0.151987</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.330768</td>\n",
       "      <td>0.168586</td>\n",
       "      <td>0.251821</td>\n",
       "      <td>0.223510</td>\n",
       "      <td>0.201043</td>\n",
       "      <td>0.019825</td>\n",
       "      <td>0.086454</td>\n",
       "      <td>0.150850</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>70</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.343225</td>\n",
       "      <td>0.168334</td>\n",
       "      <td>0.257616</td>\n",
       "      <td>0.222550</td>\n",
       "      <td>0.199619</td>\n",
       "      <td>0.020499</td>\n",
       "      <td>0.086006</td>\n",
       "      <td>0.150121</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.353402</td>\n",
       "      <td>0.165956</td>\n",
       "      <td>0.255298</td>\n",
       "      <td>0.220960</td>\n",
       "      <td>0.197152</td>\n",
       "      <td>0.020297</td>\n",
       "      <td>0.085832</td>\n",
       "      <td>0.147061</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>70</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.369101</td>\n",
       "      <td>0.165044</td>\n",
       "      <td>0.252318</td>\n",
       "      <td>0.219603</td>\n",
       "      <td>0.196109</td>\n",
       "      <td>0.020256</td>\n",
       "      <td>0.084418</td>\n",
       "      <td>0.145372</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.348436</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>0.247020</td>\n",
       "      <td>0.217119</td>\n",
       "      <td>0.196275</td>\n",
       "      <td>0.020172</td>\n",
       "      <td>0.083450</td>\n",
       "      <td>0.146095</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.373049</td>\n",
       "      <td>0.163746</td>\n",
       "      <td>0.244040</td>\n",
       "      <td>0.219073</td>\n",
       "      <td>0.195513</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>0.083868</td>\n",
       "      <td>0.144810</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.368746</td>\n",
       "      <td>0.161885</td>\n",
       "      <td>0.248179</td>\n",
       "      <td>0.211656</td>\n",
       "      <td>0.190811</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.083479</td>\n",
       "      <td>0.143423</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.366450</td>\n",
       "      <td>0.161821</td>\n",
       "      <td>0.249669</td>\n",
       "      <td>0.214735</td>\n",
       "      <td>0.191788</td>\n",
       "      <td>0.019860</td>\n",
       "      <td>0.083138</td>\n",
       "      <td>0.143416</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.386447</td>\n",
       "      <td>0.158112</td>\n",
       "      <td>0.237914</td>\n",
       "      <td>0.212285</td>\n",
       "      <td>0.188858</td>\n",
       "      <td>0.018878</td>\n",
       "      <td>0.080501</td>\n",
       "      <td>0.137558</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.400486</td>\n",
       "      <td>0.155994</td>\n",
       "      <td>0.236755</td>\n",
       "      <td>0.206722</td>\n",
       "      <td>0.187517</td>\n",
       "      <td>0.018209</td>\n",
       "      <td>0.077804</td>\n",
       "      <td>0.138427</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.395612</td>\n",
       "      <td>0.155225</td>\n",
       "      <td>0.236093</td>\n",
       "      <td>0.207649</td>\n",
       "      <td>0.186010</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.078628</td>\n",
       "      <td>0.136590</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.402303</td>\n",
       "      <td>0.154119</td>\n",
       "      <td>0.228311</td>\n",
       "      <td>0.206325</td>\n",
       "      <td>0.185679</td>\n",
       "      <td>0.016925</td>\n",
       "      <td>0.077160</td>\n",
       "      <td>0.134775</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.406481</td>\n",
       "      <td>0.152544</td>\n",
       "      <td>0.223510</td>\n",
       "      <td>0.205397</td>\n",
       "      <td>0.183493</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.077671</td>\n",
       "      <td>0.133051</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.416809</td>\n",
       "      <td>0.150068</td>\n",
       "      <td>0.226987</td>\n",
       "      <td>0.200695</td>\n",
       "      <td>0.181391</td>\n",
       "      <td>0.017459</td>\n",
       "      <td>0.075046</td>\n",
       "      <td>0.132601</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.426242</td>\n",
       "      <td>0.145972</td>\n",
       "      <td>0.220530</td>\n",
       "      <td>0.197914</td>\n",
       "      <td>0.176573</td>\n",
       "      <td>0.016214</td>\n",
       "      <td>0.073585</td>\n",
       "      <td>0.126988</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.447472</td>\n",
       "      <td>0.143657</td>\n",
       "      <td>0.225331</td>\n",
       "      <td>0.192219</td>\n",
       "      <td>0.174040</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>0.068801</td>\n",
       "      <td>0.121563</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.457236</td>\n",
       "      <td>0.139184</td>\n",
       "      <td>0.214073</td>\n",
       "      <td>0.189735</td>\n",
       "      <td>0.170281</td>\n",
       "      <td>0.015397</td>\n",
       "      <td>0.067155</td>\n",
       "      <td>0.117741</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.467740</td>\n",
       "      <td>0.136541</td>\n",
       "      <td>0.209768</td>\n",
       "      <td>0.184735</td>\n",
       "      <td>0.168063</td>\n",
       "      <td>0.015276</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>0.115223</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.473897</td>\n",
       "      <td>0.135116</td>\n",
       "      <td>0.212086</td>\n",
       "      <td>0.185331</td>\n",
       "      <td>0.165364</td>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.065727</td>\n",
       "      <td>0.112662</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.475201</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.205464</td>\n",
       "      <td>0.180199</td>\n",
       "      <td>0.163874</td>\n",
       "      <td>0.014868</td>\n",
       "      <td>0.064053</td>\n",
       "      <td>0.112968</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.483150</td>\n",
       "      <td>0.133065</td>\n",
       "      <td>0.208940</td>\n",
       "      <td>0.181589</td>\n",
       "      <td>0.163709</td>\n",
       "      <td>0.015225</td>\n",
       "      <td>0.063366</td>\n",
       "      <td>0.112042</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.505186</td>\n",
       "      <td>0.124519</td>\n",
       "      <td>0.191060</td>\n",
       "      <td>0.168742</td>\n",
       "      <td>0.151523</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.059415</td>\n",
       "      <td>0.103636</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.499758</td>\n",
       "      <td>0.124365</td>\n",
       "      <td>0.186589</td>\n",
       "      <td>0.165232</td>\n",
       "      <td>0.152748</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.058135</td>\n",
       "      <td>0.103957</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.514218</td>\n",
       "      <td>0.121368</td>\n",
       "      <td>0.190066</td>\n",
       "      <td>0.163212</td>\n",
       "      <td>0.147301</td>\n",
       "      <td>0.013704</td>\n",
       "      <td>0.056728</td>\n",
       "      <td>0.100074</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>70</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>70</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       MAP  precision@1  ...  Number of Epochs  nh  nv\n",
       "8   0.219850  0.174431     0.266225  ...                30  16   4\n",
       "48  0.199758  0.173346     0.252483  ...                70  32   2\n",
       "42  0.205151  0.172895     0.251656  ...                70  16   2\n",
       "26  0.204445  0.172397     0.253642  ...                50  16   4\n",
       "12  0.222707  0.172349     0.250000  ...                30  32   2\n",
       "32  0.202228  0.172163     0.240066  ...                50  32   4\n",
       "41  0.326347  0.172091     0.267881  ...                70   8   8\n",
       "28  0.198875  0.172074     0.258775  ...                50  16   8\n",
       "10  0.214238  0.172060     0.248510  ...                30  16   8\n",
       "14  0.217256  0.172015     0.251490  ...                30  32   4\n",
       "4   0.215491  0.171827     0.259437  ...                30   8   8\n",
       "46  0.193528  0.171692     0.245033  ...                70  16   8\n",
       "30  0.208136  0.171597     0.244536  ...                50  32   2\n",
       "34  0.195588  0.171577     0.251656  ...                50  32   8\n",
       "44  0.199088  0.171433     0.239735  ...                70  16   4\n",
       "16  0.211691  0.171403     0.243543  ...                30  32   8\n",
       "38  0.200608  0.171349     0.246026  ...                70   8   4\n",
       "2   0.222950  0.170794     0.243709  ...                30   8   4\n",
       "24  0.213339  0.170418     0.242384  ...                50  16   2\n",
       "18  0.216224  0.170389     0.248179  ...                50   8   2\n",
       "6   0.229905  0.170082     0.251325  ...                30  16   2\n",
       "0   0.234054  0.169893     0.254470  ...                30   8   2\n",
       "40  0.193464  0.169781     0.245695  ...                70   8   8\n",
       "51  0.337584  0.169671     0.258113  ...                70  32   4\n",
       "47  0.327295  0.169015     0.256954  ...                70  16   8\n",
       "53  0.330768  0.168586     0.251821  ...                70  32   8\n",
       "45  0.343225  0.168334     0.257616  ...                70  16   4\n",
       "49  0.353402  0.165956     0.255298  ...                70  32   2\n",
       "29  0.369101  0.165044     0.252318  ...                50  16   8\n",
       "39  0.348436  0.164890     0.247020  ...                70   8   4\n",
       "35  0.373049  0.163746     0.244040  ...                50  32   8\n",
       "37  0.368746  0.161885     0.248179  ...                70   8   2\n",
       "43  0.366450  0.161821     0.249669  ...                70  16   2\n",
       "23  0.386447  0.158112     0.237914  ...                50   8   8\n",
       "27  0.400486  0.155994     0.236755  ...                50  16   4\n",
       "33  0.395612  0.155225     0.236093  ...                50  32   4\n",
       "31  0.402303  0.154119     0.228311  ...                50  32   2\n",
       "21  0.406481  0.152544     0.223510  ...                50   8   4\n",
       "19  0.416809  0.150068     0.226987  ...                50   8   2\n",
       "25  0.426242  0.145972     0.220530  ...                50  16   2\n",
       "17  0.447472  0.143657     0.225331  ...                30  32   8\n",
       "11  0.457236  0.139184     0.214073  ...                30  16   8\n",
       "5   0.467740  0.136541     0.209768  ...                30   8   8\n",
       "15  0.473897  0.135116     0.212086  ...                30  32   4\n",
       "9   0.475201  0.134095     0.205464  ...                30  16   4\n",
       "3   0.483150  0.133065     0.208940  ...                30   8   4\n",
       "1   0.505186  0.124519     0.191060  ...                30   8   2\n",
       "7   0.499758  0.124365     0.186589  ...                30  16   2\n",
       "13  0.514218  0.121368     0.190066  ...                30  32   2\n",
       "36       NaN  0.005952     0.000000  ...                70   8   2\n",
       "20       NaN  0.005952     0.000000  ...                50   8   4\n",
       "22       NaN  0.005952     0.000000  ...                50   8   8\n",
       "50       NaN  0.005952     0.000000  ...                70  32   4\n",
       "52       NaN  0.005952     0.000000  ...                70  32   8\n",
       "\n",
       "[54 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the best performing hyper parameter combination\n",
    "results = pd.read_csv('results/Caser_hyperparameter.csv').drop(columns='Unnamed: 0')\n",
    "results.sort_values(by=['MAP'], ascending=False, inplace=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal hyperparameter\n",
    "lr_opt = 0.001\n",
    "epochs_opt = 30\n",
    "nh_opt = 16\n",
    "nv_opt = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training instances: 655098\n",
      "Epoch 1 [109.3 s]\tloss=0.8230 [0.0 s]\n",
      "Epoch 2 [107.8 s]\tloss=0.6174 [0.0 s]\n",
      "Epoch 3 [111.1 s]\tloss=0.5261 [0.0 s]\n",
      "Epoch 4 [112.1 s]\tloss=0.4647 [0.0 s]\n",
      "Epoch 5 [112.2 s]\tloss=0.4219 [0.0 s]\n",
      "Epoch 6 [108.8 s]\tloss=0.3902 [0.0 s]\n",
      "Epoch 7 [107.7 s]\tloss=0.3665 [0.0 s]\n",
      "Epoch 8 [108.2 s]\tloss=0.3471 [0.0 s]\n",
      "Epoch 9 [106.5 s]\tloss=0.3316 [0.0 s]\n",
      "Epoch 10 [107.3 s]\tloss=0.3177 [0.0 s]\n",
      "Epoch 11 [109.6 s]\tloss=0.3053 [0.0 s]\n",
      "Epoch 12 [108.9 s]\tloss=0.2949 [0.0 s]\n",
      "Epoch 13 [110.6 s]\tloss=0.2861 [0.0 s]\n",
      "Epoch 14 [110.8 s]\tloss=0.2770 [0.0 s]\n",
      "Epoch 15 [109.6 s]\tloss=0.2698 [0.0 s]\n",
      "Epoch 16 [108.7 s]\tloss=0.2641 [0.0 s]\n",
      "Epoch 17 [109.8 s]\tloss=0.2583 [0.0 s]\n",
      "Epoch 18 [109.8 s]\tloss=0.2524 [0.0 s]\n",
      "Epoch 19 [111.0 s]\tloss=0.2493 [0.0 s]\n",
      "Epoch 20 [109.7 s]\tloss=0.2451 [0.0 s]\n",
      "Epoch 21 [108.5 s]\tloss=0.2408 [0.0 s]\n",
      "Epoch 22 [107.4 s]\tloss=0.2373 [0.0 s]\n",
      "Epoch 23 [105.8 s]\tloss=0.2341 [0.0 s]\n",
      "Epoch 24 [100.7 s]\tloss=0.2322 [0.0 s]\n",
      "Epoch 25 [104.2 s]\tloss=0.2300 [0.0 s]\n",
      "Epoch 26 [113.9 s]\tloss=0.2283 [0.0 s]\n",
      "Epoch 27 [109.7 s]\tloss=0.2264 [0.0 s]\n",
      "Epoch 28 [110.4 s]\tloss=0.2242 [0.0 s]\n",
      "Epoch 29 [119.7 s]\tloss=0.2224 [0.0 s]\n",
      "Epoch 30 [115.9 s]\tloss=0.2212 [0.0 s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.2212129334686324,\n",
       " 'MAP': 0.17204287337960014,\n",
       " 'precision@1': 0.25579470198675497,\n",
       " 'precision@5': 0.22248344370860926,\n",
       " 'precision@10': 0.1974834437086093,\n",
       " 'recall@1': 0.021888589108570035,\n",
       " 'recall@5': 0.0908098008717107,\n",
       " 'recall@10': 0.1570444634268174,\n",
       " 'learnig rate': 0.001,\n",
       " 'Number of Epochs': 30,\n",
       " 'nh': 16,\n",
       " 'nv': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run model with optimal hyperparameters\n",
    "model_opt = Recommender(n_iter=epochs_opt,\n",
    "                        batch_size=batch_size,\n",
    "                        learning_rate=lr_opt,\n",
    "                        l2=l2,\n",
    "                        neg_samples=neg_samples,\n",
    "                        L = L,\n",
    "                        dims = d,\n",
    "                        nh = nh_opt,\n",
    "                        nv = nv_opt,\n",
    "                        drop = drop,\n",
    "                        ac_conv = ac_conv,\n",
    "                        ac_fc = ac_fc,\n",
    "                        use_cuda=use_cuda)\n",
    "\n",
    "model_opt.fit(train, val, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse build in movie mapping to make ids compareable\n",
    "movie_mapping = {v: int(k) for k, v in train.item_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unique users from the training data\n",
    "users = train_df.User.unique()\n",
    "\n",
    "# Initializing DataFrames to store results\n",
    "awhrs = pd.DataFrame()\n",
    "asats = pd.DataFrame()\n",
    "asats_2 = pd.DataFrame()\n",
    "\n",
    "# Lists to store Kendall distance sums\n",
    "kendal_sum = []\n",
    "kendal_sum_2 = []\n",
    "\n",
    "# Looping through different values of k\n",
    "for k in [1, 5, 10, 20, 50]:\n",
    "    whrs = []  # List to store Weighted Hit Rates for each user\n",
    "    sat_us = []  # List to store User Satisfaction values for each user\n",
    "    sat_us_2 = []  # List to store User Satisfaction values (with different threshold) for each user\n",
    "    recommendations_allu = []  # List to store recommendations for each user\n",
    "\n",
    "    # Looping through each user\n",
    "    for user in users:\n",
    "        whr = 0  # Initializing Weighted Hit Rate for the user\n",
    "        sat = 0  # Initializing User Satisfaction for the user\n",
    "        sat_2 = 0  # Initializing User Satisfaction (with different threshold) for the user\n",
    "\n",
    "        # Getting predictions for the current user\n",
    "        predictions_user = model_opt.predict(user - 1)\n",
    "        # Selecting top k recommendations\n",
    "        recommendations = np.argsort(predictions_user)[-k:]\n",
    "\n",
    "        # Getting ratings of recommended items for the current user\n",
    "        ratings = test[test['User'] == user]\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            # Mapping recommendations\n",
    "            rec_mapped = movie_mapping[rec]\n",
    "            rat = ratings[ratings['Movie'] == rec_mapped]\n",
    "\n",
    "            if len(rat) == 1:\n",
    "                if rat.Rating.values[0] == 1:\n",
    "                    whr -= 5\n",
    "                elif rat.Rating.values[0] == 2:\n",
    "                    whr -= 2\n",
    "                elif rat.Rating.values[0] == 3:\n",
    "                    whr += 2\n",
    "                elif rat.Rating.values[0] == 4:\n",
    "                    whr += 6\n",
    "                    sat = 1\n",
    "                elif rat.Rating.values[0] == 5:\n",
    "                    whr += 12\n",
    "                    sat = 1\n",
    "                    sat_2 = 1\n",
    "\n",
    "        whr = whr / k\n",
    "        whrs.append(whr)\n",
    "        sat_us.append(sat)\n",
    "        sat_us_2.append(sat_2)\n",
    "\n",
    "        # Store recommendations for the user\n",
    "        recommendations_allu.append(recommendations)\n",
    "\n",
    "        # Calculating Kendall Distance for the current user's predictions\n",
    "        # only once as it uses the whole sequence of predictions and is therefore independend of k\n",
    "        if k == 1:\n",
    "            predictions_user_df = pd.DataFrame({'Prediction': pd.Series(predictions_user).values,\n",
    "                                                'Movie_unmapped': pd.Series(predictions_user).index})\n",
    "            predictions_user_df['Movie'] = predictions_user_df['Movie_unmapped'].map(movie_mapping)\n",
    "\n",
    "            kendal_u = kendall_distance_with_penalty(predictions_user_df, ratings, 'Movie', 'Movie',\n",
    "                                                     'Prediction', 'Rating', p=0.05)\n",
    "            kendal_u_2 = kendall_distance_with_penalty(predictions_user_df, ratings, 'Movie', 'Movie',\n",
    "                                                       'Prediction', 'Rating', p=0.2)\n",
    "\n",
    "            kendal_sum.append(kendal_u)\n",
    "            kendal_sum_2.append(kendal_u_2)\n",
    "\n",
    "    # Calculating average Weighted Hit Rate for current k\n",
    "    average_whr = pd.DataFrame({'Average Weigthed Hit Rate': np.mean(whrs), 'k': k}, index=[0])\n",
    "    # Calculating average User Satisfaction for current k\n",
    "    average_sat = pd.DataFrame({'Average User Satisfaction': np.mean(sat_us), 'k': k}, index=[0])\n",
    "    # Calculating average User Satisfaction (with different threshold) for current k\n",
    "    average_sat_2 = pd.DataFrame({'Average User Satisfaction': np.mean(sat_us_2), 'k': k}, index=[0])\n",
    "\n",
    "    # Saving recommendation distribution for current k to a CSV file\n",
    "    recommendations_k = pd.DataFrame({'Element': pd.Series(recommendations_allu).index,\n",
    "                                      'Occurrence Count': pd.Series(recommendations_allu).values})\n",
    "    recommendations_k.to_csv(f'results/Recommendation_distribution@{k}.csv')\n",
    "\n",
    "    # Appending results for a specific k to respective DataFrames\n",
    "    awhrs = pd.concat([awhrs, average_whr], ignore_index=True)\n",
    "    asats = pd.concat([asats, average_sat], ignore_index=True)\n",
    "    asats_2 = pd.concat([asats_2, average_sat_2], ignore_index=True)\n",
    "\n",
    "# Calculating average Kendall Distances\n",
    "kendal = pd.DataFrame({'Kendall Distance': np.mean(kendal_sum), 'p': 0.05}, index=[0])\n",
    "kendal_2 = pd.DataFrame({'Kendall Distance': np.mean(kendal_sum_2), 'p': 0.2}, index=[0])\n",
    "# Concatenating both Kendall Distance DataFrames\n",
    "kendal = pd.concat([kendal, kendal_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "awhrs.to_csv('results/Caser_awhrs.csv')\n",
    "asats.to_csv('results/Caser_asats.csv')\n",
    "asats_2.to_csv('results/Caser_asats2.csv')\n",
    "kendal.to_csv('results/Caser_Kendall.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
